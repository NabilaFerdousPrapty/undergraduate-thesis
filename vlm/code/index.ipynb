{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8485663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3676018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterVQADataset(Dataset):\n",
    "    def __init__(self, images_base_dir, annotations_path, class_to_label_path, transform=None):\n",
    "        with open(annotations_path, 'r') as f:\n",
    "            self.annotations = json.load(f)\n",
    "        with open(class_to_label_path, 'r') as f:\n",
    "            self.class_to_label = json.load(f)\n",
    "        self.images_base_dir = images_base_dir\n",
    "        self.transform = transform if transform else transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "        # Filter out missing images during initialization\n",
    "        self.valid_indices = []\n",
    "        for idx, item in enumerate(self.annotations):\n",
    "            img_filename = os.path.basename(item['Image_dir'])\n",
    "            img_path = os.path.join(self.images_base_dir, img_filename)\n",
    "            if os.path.exists(img_path):\n",
    "                self.valid_indices.append(idx)\n",
    "            else:\n",
    "                print(f\"Warning: Image not found: {img_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Use valid_indices to skip missing images\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        item = self.annotations[actual_idx]\n",
    "        \n",
    "        # Extract only the image filename, ignoring folder paths in Image_dir\n",
    "        img_filename = os.path.basename(item['Image_dir'])\n",
    "        img_path = os.path.join(self.images_base_dir, img_filename)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            # Return a blank image if there's an error\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            image = torch.zeros(3, 224, 224)  # Default blank image\n",
    "\n",
    "        question = item['Question']\n",
    "        encoding = self.tokenizer(question, padding='max_length', max_length=20,\n",
    "                                  truncation=True, return_tensors='pt')\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        answer_str = item['Ground_Truth']\n",
    "        answer_token = self.class_to_label.get(answer_str, 0)  # fallback to 0\n",
    "\n",
    "        return image, input_ids, attention_mask, answer_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0472552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual encoder (CNN)\n",
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((14, 14))\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1155335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention module fusing image and question features\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, img_feat_dim, txt_feat_dim):\n",
    "        super().__init__()\n",
    "        self.img_proj = nn.Conv2d(img_feat_dim, 256, 1)\n",
    "        self.text_proj = nn.Linear(txt_feat_dim, 256)\n",
    "        self.attn_conv = nn.Conv2d(256, 1, 1)\n",
    "    \n",
    "    def forward(self, img_feats, text_feats):\n",
    "        img_proj = self.img_proj(img_feats)\n",
    "        text_proj = self.text_proj(text_feats).unsqueeze(-1).unsqueeze(-1)\n",
    "        joint = torch.tanh(img_proj + text_proj)\n",
    "        attn_scores = self.attn_conv(joint)\n",
    "        attn_weights = torch.softmax(attn_scores.view(attn_scores.size(0), -1), dim=-1)\n",
    "        attn_weights = attn_weights.view_as(attn_scores)\n",
    "        attended = (img_feats * attn_weights).sum(dim=[2, 3])\n",
    "        return attended, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d247c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAM-VQA model combining vision and language\n",
    "class SAMVQA(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.visual_encoder = VisualEncoder()\n",
    "        self.language_encoder = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.attention = AttentionModule(256, 768)  # 768 from bert pooler output\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        img_feats = self.visual_encoder(images)\n",
    "        txt_feats = self.language_encoder(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "        attended_feat, attn_weights = self.attention(img_feats, txt_feats)\n",
    "        logits = self.classifier(attended_feat)\n",
    "        return logits, attn_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72b488fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "=== DATASET ANALYSIS ===\n",
      "Actual images in directory: 1364\n",
      "Referenced images in annotations: 411\n",
      "Annotations count: 1833\n",
      "\n",
      "Missing images: 411\n",
      "Extra images (not referenced): 1364\n",
      "\n",
      "Sample of missing images (first 10):\n",
      "  - 7880.JPG\n",
      "  - 7143.JPG\n",
      "  - 8797.JPG\n",
      "  - 9231.JPG\n",
      "  - 7431.JPG\n",
      "  - 7232.JPG\n",
      "  - 6594.JPG\n",
      "  - 9344.JPG\n",
      "  - 7876.JPG\n",
      "  - 6898.JPG\n",
      "\n",
      "‚ùå Dataset has missing images. Please fix the issues above before training.\n",
      "\n",
      "Possible solutions:\n",
      "1. Download the missing images\n",
      "2. Check if images are in subdirectories\n",
      "3. Fix the Image_dir paths in your JSON file\n",
      "4. Use case-correct filenames\n"
     ]
    }
   ],
   "source": [
    "def analyze_dataset_issues(images_base_dir, annotations_path):\n",
    "    \"\"\"Analyze what's wrong with the dataset\"\"\"\n",
    "    print(\"\\n=== DATASET ANALYSIS ===\")\n",
    "    \n",
    "    # Load annotations\n",
    "    with open(annotations_path, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    # Get actual image files in directory\n",
    "    actual_images = set([f for f in os.listdir(images_base_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "    \n",
    "    # Get referenced images from annotations\n",
    "    referenced_images = set()\n",
    "    for item in annotations:\n",
    "        img_filename = os.path.basename(item['Image_dir'])\n",
    "        referenced_images.add(img_filename)\n",
    "    \n",
    "    print(f\"Actual images in directory: {len(actual_images)}\")\n",
    "    print(f\"Referenced images in annotations: {len(referenced_images)}\")\n",
    "    print(f\"Annotations count: {len(annotations)}\")\n",
    "    \n",
    "    # Find missing images\n",
    "    missing = referenced_images - actual_images\n",
    "    print(f\"\\nMissing images: {len(missing)}\")\n",
    "    \n",
    "    # Find extra images (not referenced)\n",
    "    extra = actual_images - referenced_images\n",
    "    print(f\"Extra images (not referenced): {len(extra)}\")\n",
    "    \n",
    "    # Show sample of missing images\n",
    "    if missing:\n",
    "        print(f\"\\nSample of missing images (first 10):\")\n",
    "        for img in list(missing)[:10]:\n",
    "            print(f\"  - {img}\")\n",
    "    \n",
    "    # Check for case sensitivity issues\n",
    "    actual_images_lower = set([f.lower() for f in actual_images])\n",
    "    missing_due_to_case = []\n",
    "    for ref_img in referenced_images:\n",
    "        if ref_img not in actual_images and ref_img.lower() in actual_images_lower:\n",
    "            missing_due_to_case.append(ref_img)\n",
    "    \n",
    "    if missing_due_to_case:\n",
    "        print(f\"\\nPossible case sensitivity issues ({len(missing_due_to_case)} images):\")\n",
    "        for img in missing_due_to_case[:5]:\n",
    "            print(f\"  - {img} (referenced) vs actual: {[f for f in actual_images if f.lower() == img.lower()]}\")\n",
    "    \n",
    "    return len(missing) == 0\n",
    "\n",
    "\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Use your actual paths here!\n",
    "    images_base_dir = \"data/Images/train_images\"\n",
    "    annotations_path = \"data/data/train_annotations.json\"\n",
    "    class_to_label_path = \"data/data/class_to_label.json\"\n",
    "\n",
    "    # First, analyze what's wrong\n",
    "    all_images_exist = analyze_dataset_issues(images_base_dir, annotations_path)\n",
    "    \n",
    "    if not all_images_exist:\n",
    "        print(\"\\n‚ùå Dataset has missing images. Please fix the issues above before training.\")\n",
    "        print(\"\\nPossible solutions:\")\n",
    "        print(\"1. Download the missing images\")\n",
    "        print(\"2. Check if images are in subdirectories\")\n",
    "        print(\"3. Fix the Image_dir paths in your JSON file\")\n",
    "        print(\"4. Use case-correct filenames\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        dataset = DisasterVQADataset(images_base_dir, annotations_path, class_to_label_path)\n",
    "        print(f\"Dataset loaded with {len(dataset)} valid samples out of {len(dataset.annotations)} total\")\n",
    "        \n",
    "        if len(dataset) == 0:\n",
    "            print(\"No valid samples found! Check your image paths.\")\n",
    "            return\n",
    "            \n",
    "        dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "        with open(class_to_label_path, 'r') as f:\n",
    "            class_to_label = json.load(f)\n",
    "        num_classes = len(class_to_label)\n",
    "        print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "        model = SAMVQA(num_classes).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        print(\"Starting training...\")\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            batch_count = 0\n",
    "            \n",
    "            for batch_idx, (images, input_ids, attention_mask, answers) in enumerate(dataloader):\n",
    "                images = images.to(device)\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                answers = answers.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits, _ = model(images, input_ids, attention_mask)\n",
    "                loss = criterion(logits, answers)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                \n",
    "                if batch_idx % 10 == 0:  # Print progress every 10 batches\n",
    "                    print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            avg_loss = total_loss / batch_count\n",
    "            print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
